TODO

Adagrad, Adadelta, Conjugate gradient descent?

Make it so dloss() can be a function of more than just h

Add a softmax layer for the output of the LSTM

Perform BPTT on LSTM using subsequences of size k

Implement regularization

Make it so you don't forward propagate in both backprop_once() and BPTT()
